{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gt-cse-6040/bootcamp/blob/main/Module%201/Session%209/m1s9nb2_Data_Debugging_part2%20-%20Solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2ba5485-42c1-4186-97d1-29635d12e51f",
      "metadata": {
        "id": "a2ba5485-42c1-4186-97d1-29635d12e51f"
      },
      "source": [
        "# Debugging Bad Solutions: Module 1, Part 2\n",
        "\n",
        "From Midterm 2, Fall 2022 - Two and Three-Point Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cf1e752-a17d-4a8d-a7b5-fdfbb220a55e",
      "metadata": {
        "id": "7cf1e752-a17d-4a8d-a7b5-fdfbb220a55e"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dill\n",
        "\n",
        "!mkdir resources\n",
        "%cd resources\n",
        "\n",
        "!wget https://raw.githubusercontent.com/gt-cse-6040/bootcamp/main/Module%201/Session%209/resources/tc_1\n",
        "!wget https://raw.githubusercontent.com/gt-cse-6040/bootcamp/main/Module%201/Session%209/resources/tc_3\n",
        "!wget https://raw.githubusercontent.com/gt-cse-6040/bootcamp/main/Module%201/Session%209/resources/tc_5\n",
        "\n",
        "%cd ..\n",
        "\n",
        "!mkdir tester_fw\n",
        "%cd tester_fw\n",
        "\n",
        "!wget https://raw.githubusercontent.com/gt-cse-6040/bootcamp/main/Module%201/Session%209/tester_fw/__init__.py\n",
        "!wget https://raw.githubusercontent.com/gt-cse-6040/bootcamp/main/Module%201/Session%209/tester_fw/test_utils.py\n",
        "!wget https://raw.githubusercontent.com/gt-cse-6040/bootcamp/main/Module%201/Session%209/tester_fw/testers.py\n",
        "\n",
        "%cd .."
      ],
      "metadata": {
        "id": "egSywBHX93WF"
      },
      "id": "egSywBHX93WF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "255cf4c3-79da-4c2d-a45a-775d011878f4",
      "metadata": {
        "id": "255cf4c3-79da-4c2d-a45a-775d011878f4"
      },
      "source": [
        "### Purpose\n",
        "\n",
        "On the exams you may initially write solutions that do not pass the test cases. That's okay! You will need to debug your code to determine what is causing the issue(s) and then figure out to how fix them. So how can we get better at debugging? We practice!\n",
        "\n",
        "Below are exercises from the Fall 2022 Midterm 2. We have pre-written solutions for each exercise that are \"bad\" in one or more ways. Our solutions may contain one or more logic and/or syntax errors. . Can you find and fix the issues in each exercise and pass all of the test cases?\n",
        "\n",
        "**Debugging your code:** Right before each exercise test cell, there is a block of text explaining the variables available to you for debugging. You may use these to test your code and can print/display them as needed (careful when printing large objects, you may want to print the head or chunks of rows at a time).\n",
        "\n",
        "**Exercise point breakdown:**\n",
        "\n",
        "- Exercise 1: **2** point(s)\n",
        "- Exercise 3: **2** point(s)\n",
        "- Exercise 5: **3** point(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbbff24b-69b7-407a-9b34-aaedfc437123",
      "metadata": {
        "id": "dbbff24b-69b7-407a-9b34-aaedfc437123"
      },
      "source": [
        "## Scenario (don't dwell on this)\n",
        "\n",
        "You have just been hired by the hot new startup **Spot-i-flix-ify** (this is a fictional company which will offer video and audio streaming services) as a Data Scientist. This is a small startup so you have to \"wear many different hats,\" so to speak. Your first task on the job is to set up their data warehousing so that they can capture a historical record of their operations for analysis later. The operational database (which someone else has already set up) only contains the current state of the operation to maintain maximum efficiency while performing tasks like adding new customers, changing services, applying promotions, etc. It will not contain any history and is not intended have complex queries run against it.\n",
        "  \n",
        "While this is a fictional company and simulation data, **there is a real-world use case** for the processes developed in this notebook.\n",
        "\n",
        "## On data types\n",
        "These tables are made available to you in a staging environment as Pandas `DataFrame` objects. **All columns in all of the DataFrames are strings (even the columns where you would expect other data types)**.\n",
        "\n",
        "## On SQL\n",
        "We used Pandas exclusively in developing this problem, however some exercise are solvable using SQL. In the cell below we have included the function `dfs_to_conn` which can be used to create in-memory database connections. If you pass in a dictionary mapping table names to DataFrames, `dfs_to_conn` will return a sqlite 3 connection with all of the data in the DataFrames available under the names given as keys. You are also free to write to the in-memory database by creating tables, inserting/deleting/updating records, etc. Anything that SQLite allows should work!\n",
        "\n",
        "Example:\n",
        "  ```\n",
        "my_df = pd.DataFrame({'A':[1,2,3], 'B': [4,5,6], 'C':['x', 'y', 'z']})\n",
        "print(my_df)\n",
        "#    A  B  C\n",
        "# 0  1  4  x\n",
        "# 1  2  5  y\n",
        "# 2  3  6  z\n",
        "conn = dfs_to_conn({'my_table': my_df})\n",
        "cur = conn.cursor()\n",
        "cur.execute('select A, B, C from my_table')\n",
        "result = cur.fetchall()\n",
        "conn.close()\n",
        "print(result) # list of tuples, each tuple is a row\n",
        "#[(1, 4, 'x'), (2, 5, 'y'), (3, 6, 'z')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdc3fea7-7492-4ea8-81b6-d1181df83c60",
      "metadata": {
        "id": "cdc3fea7-7492-4ea8-81b6-d1181df83c60"
      },
      "outputs": [],
      "source": [
        "### Global Imports\n",
        "###\n",
        "### AUTOGRADER TEST - DO NOT REMOVE\n",
        "###\n",
        "import pandas as pd\n",
        "import time\n",
        "overall_start = time.time()\n",
        "\n",
        "def dfs_to_conn(conn_dfs, index=False):\n",
        "    import sqlite3\n",
        "    conn = sqlite3.connect(':memory:')\n",
        "    for table_name, df in conn_dfs.items():\n",
        "        df.to_sql(table_name, conn, if_exists='replace', index=index)\n",
        "    return conn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeadb4ba-08aa-4261-8836-63a882f4d781",
      "metadata": {
        "scrolled": true,
        "id": "aeadb4ba-08aa-4261-8836-63a882f4d781",
        "outputId": "5441205b-f027-4c0a-e040-26cd1317155e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: asttokens==2.4.1 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from -r resources/requirements.txt (line 1)) (2.4.1)\n",
            "Requirement already satisfied: cffi==1.16.0 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from -r resources/requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: comm==0.2.0 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from -r resources/requirements.txt (line 3)) (0.2.0)\n",
            "Requirement already satisfied: cryptography==41.0.7 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from -r resources/requirements.txt (line 4)) (41.0.7)\n",
            "Requirement already satisfied: debugpy==1.8.0 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from -r resources/requirements.txt (line 5)) (1.8.0)\n",
            "Requirement already satisfied: decorator==5.1.1 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from -r resources/requirements.txt (line 6)) (5.1.1)\n",
            "Requirement already satisfied: dill==0.3.7 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from -r resources/requirements.txt (line 7)) (0.3.7)\n",
            "Requirement already satisfied: exceptiongroup==1.2.0 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from -r resources/requirements.txt (line 8)) (1.2.0)\n",
            "Requirement already satisfied: executing==2.0.1 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from -r resources/requirements.txt (line 9)) (2.0.1)\n",
            "Requirement already satisfied: ipykernel==6.27.1 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from -r resources/requirements.txt (line 10)) (6.27.1)\n",
            "Requirement already satisfied: ipython==8.18.1 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from -r resources/requirements.txt (line 11)) (8.18.1)\n",
            "Requirement already satisfied: jedi==0.19.1 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from -r resources/requirements.txt (line 12)) (0.19.1)\n",
            "Requirement already satisfied: jupyter_client==8.6.0 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from -r resources/requirements.txt (line 13)) (8.6.0)\n",
            "Requirement already satisfied: jupyter_core==5.5.0 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from -r resources/requirements.txt (line 14)) (5.5.0)\n",
            "Requirement already satisfied: matplotlib-inline==0.1.6 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from -r resources/requirements.txt (line 15)) (0.1.6)\n",
            "Requirement already satisfied: nest-asyncio==1.5.8 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from -r resources/requirements.txt (line 16)) (1.5.8)\n",
            "Requirement already satisfied: networkx==3.2.1 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from -r resources/requirements.txt (line 17)) (3.2.1)\n",
            "Requirement already satisfied: numpy==1.26.2 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from -r resources/requirements.txt (line 18)) (1.26.2)\n",
            "Requirement already satisfied: packaging==23.2 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from -r resources/requirements.txt (line 19)) (23.2)\n",
            "Requirement already satisfied: pandas==1.5.3 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from -r resources/requirements.txt (line 20)) (1.5.3)\n",
            "Requirement already satisfied: parso==0.8.3 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from -r resources/requirements.txt (line 21)) (0.8.3)\n",
            "Requirement already satisfied: pexpect==4.9.0 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from -r resources/requirements.txt (line 22)) (4.9.0)\n",
            "Requirement already satisfied: platformdirs==4.1.0 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from -r resources/requirements.txt (line 23)) (4.1.0)\n",
            "Requirement already satisfied: prompt-toolkit==3.0.41 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from -r resources/requirements.txt (line 24)) (3.0.41)\n",
            "Requirement already satisfied: psutil==5.9.6 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from -r resources/requirements.txt (line 25)) (5.9.6)\n",
            "Requirement already satisfied: ptyprocess==0.7.0 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from -r resources/requirements.txt (line 26)) (0.7.0)\n",
            "Requirement already satisfied: pure-eval==0.2.2 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from -r resources/requirements.txt (line 27)) (0.2.2)\n",
            "Requirement already satisfied: pycparser==2.21 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from -r resources/requirements.txt (line 28)) (2.21)\n",
            "Requirement already satisfied: Pygments==2.17.2 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from -r resources/requirements.txt (line 29)) (2.17.2)\n",
            "Requirement already satisfied: python-dateutil==2.8.2 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from -r resources/requirements.txt (line 30)) (2.8.2)\n",
            "Requirement already satisfied: pytz==2023.3.post1 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from -r resources/requirements.txt (line 31)) (2023.3.post1)\n",
            "Requirement already satisfied: pyzmq==25.1.2 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from -r resources/requirements.txt (line 32)) (25.1.2)\n",
            "Requirement already satisfied: six==1.16.0 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from -r resources/requirements.txt (line 33)) (1.16.0)\n",
            "Requirement already satisfied: stack-data==0.6.3 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from -r resources/requirements.txt (line 34)) (0.6.3)\n",
            "Requirement already satisfied: tornado==6.4 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from -r resources/requirements.txt (line 35)) (6.4)\n",
            "Requirement already satisfied: traitlets==5.14.0 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from -r resources/requirements.txt (line 36)) (5.14.0)\n",
            "Requirement already satisfied: wcwidth==0.2.12 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from -r resources/requirements.txt (line 37)) (0.2.12)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from ipython==8.18.1->-r resources/requirements.txt (line 11)) (4.9.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from ipython==8.18.1->-r resources/requirements.txt (line 11)) (0.4.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.3 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from jupyter_client==8.6.0->-r resources/requirements.txt (line 13)) (7.0.0)\n",
            "Requirement already satisfied: pywin32>=300 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from jupyter_core==5.5.0->-r resources/requirements.txt (line 14)) (306)\n",
            "Requirement already satisfied: zipp>=0.5 in c:\\users\\nawid\\documents\\cse 6040\\bootcamp\\module 1 session 9\\venv\\lib\\site-packages (from importlib-metadata>=4.8.3->jupyter_client==8.6.0->-r resources/requirements.txt (line 13)) (3.17.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.1.2; however, version 23.3.1 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\nawid\\Documents\\CSE 6040\\Bootcamp\\Module 1 Session 9\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install -r resources/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f2bf174-82b0-4c8f-8985-df2a60da800c",
      "metadata": {
        "id": "5f2bf174-82b0-4c8f-8985-df2a60da800c"
      },
      "source": [
        "## Data (Don't dwell on this. The structures you are working with will be explained in each exercise.)\n",
        "You are working with four tables:\n",
        "- `customers` - Center of the \"star\" schema. Primary Key: `id`.\n",
        "  - `customers.id` - a unique identifier for an individual customer.\n",
        "  - `customers.paid` - ('True'|'False') - indicates whether a customer has paid their bill for their upcoming month of service.\n",
        "- `prices` - The prices of the services offered by **Spot-i-flix-ify**. Primary Key: `service, tier, promo`\n",
        "  - `prices.service` - Name of the sevice\n",
        "  - `prices.tier` - Tier of the service. A service can be offered in several tiers. Higher tiers give customers more features.\n",
        "  - `prices.promo` - Promotion which can be applied to a service/tier combination to offer a discount to customers.\n",
        "  - `prices.price` - The price of a particular service/tier/promo combination.\n",
        "- `services` - Services which each customer is subscribed. Primary Key: `cust_id, service`; Foreign Keys: `cust_id` references `customers.id`, (`service, tier`) references (`prices.service, prices.tier`)\n",
        "  - `services.cust_id` - id of the customer associated with this subscription.\n",
        "  - `services.service` - name of service associated with this subscription.\n",
        "  - `services.tier` - tier of service associated with a subscription.\n",
        "- `active_promos` - the promotion which is actually applied to each customer for a particular service\n",
        "  - `'cust_id'` - identifies an individual customer.\n",
        "  - `'service'` - identifies a service for which the customer has an active promotion. **The customer may not actually be subscribed to the service!**\n",
        "  - `'promo'` - the active promo for the `cust_id`/`service` pair.\n",
        "    - If `'time_left'` is '0' for all records associated with the `cust_id`/`service` pair in `promos`, this column should have a value of `'base'`.\n",
        "    - If there is a record associated with a non-zero `'time_left'`, this column should have the `'promo'` from that record."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d134218f-6a49-4b4d-ad72-19375c0dabd3",
      "metadata": {
        "id": "d134218f-6a49-4b4d-ad72-19375c0dabd3"
      },
      "source": [
        "## Exercise 1 - (**2** Points):  \n",
        "\n",
        "**Requirements**:  \n",
        "Define the function `denormalize(customers, services, active_promos, prices)` which takes the DataFrame inputs defined above. See the data model diagram for the relationships between the 4 tables.\n",
        "\n",
        "![data_model](https://github.com/gt-cse-6040/bootcamp/blob/main/Module%201/Session%209/resources/schema.png?raw=1)\n",
        "\n",
        "  Your function should return a DataFrame `df` which contains the following columns:\n",
        "  - `id` - identifies a particular customer (from `customers`)\n",
        "  - `paid` - ('True'|'False') indicating whether the customer `id` has paid their bill (from `customers`)\n",
        "  - `service` - a service which a customer is subscribed. There should be one record for each unique `id`/`service` pair (from `services`)\n",
        "  - `tier` - tier of a service for the `id`/`service` pair. (from `services`)\n",
        "  - `promo` - promo being applied to the `id`/`service` pair. (from `active_promos`)\n",
        "      - Remember that a record existing with a `cust_id`/`service` combination in `active_promos` _does not imply the customer is subscribed to that service_.\n",
        "  - `price` - price charged for the `id`/`service` pair (from `prices`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7470406-802a-48be-b316-ae4be04734af",
      "metadata": {
        "id": "a7470406-802a-48be-b316-ae4be04734af",
        "outputId": "22f79077-f8ee-4859-891a-eb34210ff792"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "customers\n",
            "  id   paid\n",
            "0  0   True\n",
            "1  1  False\n",
            "\n",
            "services\n",
            "  cust_id service tier\n",
            "0       0   audio    1\n",
            "1       1   video    1\n",
            "2       1   audio    2\n",
            "\n",
            "active_promos\n",
            "  cust_id service  promo\n",
            "0       0   audio  intro\n",
            "1       0   video   base\n",
            "2       1   audio   base\n",
            "3       1   video  intro\n",
            "\n",
            "prices\n",
            "  service tier  promo  price\n",
            "0   audio    1   base   8.99\n",
            "1   audio    1  intro   5.99\n",
            "2   audio    2   base  12.99\n",
            "3   audio    2  intro   9.99\n",
            "4   video    1   base  10.99\n",
            "5   video    1  intro   8.99\n",
            "6   video    2   base  15.99\n",
            "7   video    2  intro  11.99\n"
          ]
        }
      ],
      "source": [
        "### Define demo inputs\n",
        "\n",
        "demo_customers_ex1 = pd.DataFrame({'id': {0: '0', 1: '1'}, 'paid': {0: 'True', 1: 'False'}})\n",
        "demo_active_promos_ex1 = pd.DataFrame({'cust_id': {0: '0', 1: '0', 2: '1', 3: '1'},\n",
        " 'service': {0: 'audio', 1: 'video', 2: 'audio', 3: 'video'},\n",
        " 'promo': {0: 'intro', 1: 'base', 2: 'base', 3: 'intro'}})\n",
        "demo_prices_ex1 = pd.DataFrame(\n",
        "    {'service': {0: 'audio',  1: 'audio',  2: 'audio',  3: 'audio',  4: 'video',  5: 'video',  6: 'video',  7: 'video'},\n",
        "    'tier': {0: '1', 1: '1', 2: '2', 3: '2', 4: '1', 5: '1', 6: '2', 7: '2'},\n",
        "    'promo': {0: 'base', 1: 'intro', 2: 'base', 3: 'intro', 4: 'base', 5: 'intro', 6: 'base', 7: 'intro'},\n",
        "    'price': {0: '8.99', 1: '5.99', 2: '12.99', 3: '9.99', 4: '10.99', 5: '8.99', 6: '15.99', 7: '11.99'}})\n",
        "demo_services_ex1 = pd.DataFrame({'cust_id': {0: '0', 1: '1', 2: '1'},\n",
        " 'service': {0: 'audio', 1: 'video', 2: 'audio'},\n",
        " 'tier': {0: '1', 1: '1', 2: '2'}})\n",
        "\n",
        "print('customers')\n",
        "print(demo_customers_ex1)\n",
        "print()\n",
        "print('services')\n",
        "print(demo_services_ex1)\n",
        "print()\n",
        "print('active_promos')\n",
        "print(demo_active_promos_ex1)\n",
        "print()\n",
        "print('prices')\n",
        "print(demo_prices_ex1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0ea2520-9f84-4092-85ff-bc0031a6226b",
      "metadata": {
        "id": "d0ea2520-9f84-4092-85ff-bc0031a6226b"
      },
      "outputs": [],
      "source": [
        "outline = '''\n",
        "output:\n",
        "    - df\n",
        "        - data_type == DataFrame\n",
        "        - cols:\n",
        "            - customers.id, customers.paid\n",
        "            - services.service, services.tier\n",
        "            - active_promos.promo of a subscribed id/service pair\n",
        "            - prices.price of an id/service pair\n",
        "        - grain == one row per cust_id + service\n",
        "        - column types are strings\n",
        "inputs:\n",
        "    - customers, services, active_promos, prices\n",
        "        - data_type == DataFrame\n",
        "steps:\n",
        "    1. we want to return all customers + the services they're currently paying for\n",
        "        - JOIN between customers and services on customer_id\n",
        "    2. we want the active promo for the current services\n",
        "        - JOIN between services and active_promos on customer_id AND service\n",
        "    3. we want the prices for the service/tier/promo combo\n",
        "        - JOIN on all 3 of the fields\n",
        "    4. convert column types to string\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab60d0c3-c8fe-4c74-9464-02ddfb863bd4",
      "metadata": {
        "id": "ab60d0c3-c8fe-4c74-9464-02ddfb863bd4",
        "outputId": "52e25604-7165-4602-a044-863ce3ecc945"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  id   paid service tier  promo  price\n",
            "0  0   True   audio    1  intro   5.99\n",
            "1  1  False   audio    2   base  12.99\n",
            "2  1  False   video    1  intro   8.99\n"
          ]
        }
      ],
      "source": [
        "### Exercise 1 solution\n",
        "def denormalize(customers, services, active_promos, prices):\n",
        "    ###\n",
        "    ### YOUR CODE HERE\n",
        "    ###\n",
        "\n",
        "    conn = dfs_to_conn({'customers': customers, 'services': services, 'active_promos': active_promos, 'prices': prices})\n",
        "    query = '''\n",
        "    SELECT\n",
        "        c.id\n",
        "      , c.paid\n",
        "      , s.service\n",
        "      , s.tier\n",
        "      , ap.promo\n",
        "      , p.price\n",
        "    FROM customers c\n",
        "    JOIN services s\n",
        "      ON c.id = s.cust_id\n",
        "    JOIN active_promos ap\n",
        "      ON c.id = ap.cust_id\n",
        "      AND s.service = ap.service\n",
        "    JOIN prices p\n",
        "      ON s.service = p.service\n",
        "      AND s.tier = p.tier\n",
        "      AND ap.promo = p.promo\n",
        "    '''\n",
        "\n",
        "    df = pd.read_sql_query(query, conn)\n",
        "    df = df.astype('string')\n",
        "    return df\n",
        "\n",
        "demo_ex1_output = denormalize(demo_customers_ex1, demo_services_ex1, demo_active_promos_ex1, demo_prices_ex1)\n",
        "print(demo_ex1_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5edcbc0-8dfa-4279-b75e-e529967cfdde",
      "metadata": {
        "id": "d5edcbc0-8dfa-4279-b75e-e529967cfdde"
      },
      "source": [
        "<!-- Test Cell Boilerplate -->\n",
        "The cell below will test your solution for Exercise 1. The testing variables will be available for debugging under the following names in a dictionary format.\n",
        "- `input_vars` - Input variables for your solution.\n",
        "- `original_input_vars` - Copy of input variables from prior to running your solution. These _should_ be the same as `input_vars` - otherwise the inputs were modified by your solution.\n",
        "- `returned_output_vars` - Outputs returned by your solution.\n",
        "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55c21db1-7ba5-4638-a164-58ba9915824a",
      "metadata": {
        "id": "55c21db1-7ba5-4638-a164-58ba9915824a",
        "outputId": "13282947-211b-4c2e-b2dd-bf046dcc7c54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This test executed in 3 seconds\n",
            "Passed! Please submit.\n"
          ]
        }
      ],
      "source": [
        "### test_cell_ex1\n",
        "\n",
        "exercise_start = time.time()\n",
        "from tester_fw.testers import Tester\n",
        "\n",
        "conf = {\n",
        "    'case_file':'tc_1',\n",
        "    'func': denormalize, # replace this with the function defined above\n",
        "    'inputs':{ # input config dict. keys are parameter names\n",
        "        'customers':{\n",
        "            'dtype':'pd.DataFrame', # data type of param.\n",
        "            'check_modified':True,\n",
        "        },\n",
        "        'services':{\n",
        "            'dtype':'pd.DataFrame', # data type of param.\n",
        "            'check_modified':True,\n",
        "        },\n",
        "        'active_promos':{\n",
        "            'dtype':'pd.DataFrame', # data type of param.\n",
        "            'check_modified':True,\n",
        "        },\n",
        "        'prices':{\n",
        "            'dtype':'pd.DataFrame', # data type of param.\n",
        "            'check_modified':True,\n",
        "        }\n",
        "    },\n",
        "    'outputs':{\n",
        "        'output_0':{\n",
        "            'index':0,\n",
        "            'dtype':'',\n",
        "            'check_dtype': True,\n",
        "            'check_col_dtypes': True, # Ignored if dtype is not df\n",
        "            'check_col_order': False, # Ignored if dtype is not df\n",
        "            'check_row_order': False, # Ignored if dtype is not df\n",
        "            'check_column_type': True, # Ignored if dtype is not df\n",
        "            'float_tolerance': 10 ** (-6)\n",
        "        }\n",
        "    }\n",
        "}\n",
        "tester = Tester(conf, key=b'6IRWMcPsVIAZqzDJnPgv_MfUZsxqo4Utjm2Favidv-A=', path='resources/')\n",
        "for _ in range(70):\n",
        "    try:\n",
        "        tester.run_test()\n",
        "        (input_vars, original_input_vars, returned_output_vars, true_output_vars) = tester.get_test_vars()\n",
        "    except:\n",
        "        (input_vars, original_input_vars, returned_output_vars, true_output_vars) = tester.get_test_vars()\n",
        "        raise\n",
        "\n",
        "exercise_end = time.time()\n",
        "print(f\"This test executed in {(pd.to_datetime(exercise_end, unit='s') - pd.to_datetime(exercise_start, unit='s')).seconds} seconds\")\n",
        "print('Passed! Please submit.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6639cf5-afd8-44bf-92f1-3955e3308efb",
      "metadata": {
        "id": "b6639cf5-afd8-44bf-92f1-3955e3308efb"
      },
      "source": [
        "## Exercise 3 - (**2** Points):  \n",
        "**Motivation** (don't dwell on this):  \n",
        "\n",
        "The first task in our journaling process is to identify which records in the existing journal are active and which records are not. We will do so by checking the `'exp_dt'` column. All records with `'9999-12-31'` as their expiration date are considered active. We will be rebuilding the entire journal, so we need to partition the existing journal into active and not-active records and return both parts. The active records will be compared with the business data, and the inactive records will be included in the updated journal without modification. Additionally, on the initial load, there will not be an existing journal, so we will need to create it based on the data being loaded and the desired audit columns.\n",
        "\n",
        "**Requirements**:  \n",
        "Define `partition_journal(df, audit_cols, existing_journal=None)`.\n",
        "\n",
        "- The input `df` is a DataFrame - we do not care about it's structure.\n",
        "- The input `audit_cols` is a `list` of strings. These are the names of audit columns used to track history in the journal. `audit_cols` will always include the strings `'eff_dt'` and `'exp_dt'`.\n",
        "- The optional input `existing_journal` is a DataFrame or `None`.  If `existing_journal` is not `None` it will have all of the columns in `df` and all of the `audit_cols` as its columns.\n",
        "\n",
        "Your function should do the following:\n",
        "- If `existing_journal` is `None`, create an empty DataFrame which has all of the columns in `df` and all of the `audit_cols` as its columns. This empty DataFrame will be used in the subsequent operations.\n",
        "- Create `historical_journal` which is a DataFrame containing all rows of `existing_journal` where `'exp_dt'` is something other than `'9999-12-31'`.\n",
        "- Create `active_journal` which is a DataFrame containing all rows of `existing_journal` where `'exp_dt'` is `'9999-12-31'`.\n",
        "- Return the tuple `(historical_journal, active_journal)` - If the `existing_journal` was newly created these will be two empty DataFrames with all columns present in `df` and all of the `audit_cols`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "686d11b1-5277-40fa-aa05-c58f8a09e89e",
      "metadata": {
        "id": "686d11b1-5277-40fa-aa05-c58f8a09e89e",
        "outputId": "9271c53f-39a9-4b97-d09a-2e8d17b4685e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "df\n",
            "  id  paid service tier promo  price\n",
            "0  1  True   audio    1  base   8.99\n",
            "1  2  True   video    2  base  15.99\n",
            "2  2  True   audio    2  base  12.99\n",
            "\n",
            "audit_cols\n",
            "['eff_dt', 'exp_dt']\n",
            "\n",
            "existing_journal\n",
            "     id   paid service tier  promo  price      eff_dt      exp_dt\n",
            "667   0   True   video    2  intro  11.99  2018-02-01  2018-07-31\n",
            "668   1   True   audio    1  intro   5.99  2018-02-01  2018-07-31\n",
            "669   2   True   video    2  intro  11.99  2018-02-01  2018-07-31\n",
            "670   2   True   audio    2  intro   9.99  2018-02-01  2018-07-31\n",
            "671   3   True   video    1  intro   8.99  2018-02-01  2018-07-31\n",
            "672   3   True   audio    1  intro   5.99  2018-02-01  2018-07-31\n",
            "673   4   True   audio    1  intro   5.99  2018-02-01  2018-07-31\n",
            "9     3  False   video    1   base  10.99  2018-08-01  2018-08-31\n",
            "10    3  False   audio    1   base   8.99  2018-08-01  2018-08-31\n",
            "17    0   True   video    2   base  15.99  2018-08-01  2019-02-28\n",
            "1881  1   True   audio    1   base   8.99  2018-08-01  9999-12-31\n",
            "1882  2   True   video    2   base  15.99  2018-08-01  9999-12-31\n",
            "1883  2   True   audio    2   base  12.99  2018-08-01  9999-12-31\n",
            "1884  4   True   audio    1   base   8.99  2018-08-01  9999-12-31\n"
          ]
        }
      ],
      "source": [
        "### Define demo inputs\n",
        "\n",
        "demo_df_ex3 = pd.DataFrame({'id': {0: '1', 1: '2', 2: '2'},\n",
        " 'paid': {0: 'True', 1: 'True', 2: 'True'},\n",
        " 'service': {0: 'audio', 1: 'video', 2: 'audio'},\n",
        " 'tier': {0: '1', 1: '2', 2: '2'},\n",
        " 'promo': {0: 'base', 1: 'base', 2: 'base'},\n",
        " 'price': {0: '8.99', 1: '15.99', 2: '12.99'}})\n",
        "demo_existing_journal_ex3 = pd.DataFrame(\n",
        "    {'id': {667: '0', 668: '1', 669: '2', 670: '2', 671: '3', 672: '3', 673: '4', 9: '3', 10: '3', 17: '0', 1881: '1', 1882: '2', 1883: '2', 1884: '4'},\n",
        "    'paid': {667: 'True', 668: 'True', 669: 'True', 670: 'True', 671: 'True', 672: 'True', 673: 'True', 9: 'False', 10: 'False', 17: 'True',\n",
        "            1881: 'True', 1882: 'True', 1883: 'True', 1884: 'True'},\n",
        "    'service': {667: 'video', 668: 'audio', 669: 'video', 670: 'audio', 671: 'video', 672: 'audio', 673: 'audio', 9: 'video', 10: 'audio', 17: 'video',\n",
        "            1881: 'audio', 1882: 'video', 1883: 'audio', 1884: 'audio'},\n",
        "    'tier': {667: '2', 668: '1', 669: '2', 670: '2', 671: '1', 672: '1', 673: '1', 9: '1', 10: '1', 17: '2', 1881: '1', 1882: '2', 1883: '2', 1884: '1'},\n",
        "    'promo': {667: 'intro', 668: 'intro', 669: 'intro', 670: 'intro', 671: 'intro', 672: 'intro', 673: 'intro', 9: 'base', 10: 'base', 17: 'base',\n",
        "            1881: 'base', 1882: 'base', 1883: 'base', 1884: 'base'},\n",
        "    'price': {667: '11.99', 668: '5.99', 669: '11.99', 670: '9.99', 671: '8.99', 672: '5.99', 673: '5.99', 9: '10.99', 10: '8.99', 17: '15.99',\n",
        "            1881: '8.99', 1882: '15.99', 1883: '12.99', 1884: '8.99'},\n",
        "    'eff_dt': {667: '2018-02-01', 668: '2018-02-01', 669: '2018-02-01', 670: '2018-02-01', 671: '2018-02-01', 672: '2018-02-01', 673: '2018-02-01',\n",
        "            9: '2018-08-01', 10: '2018-08-01', 17: '2018-08-01', 1881: '2018-08-01', 1882: '2018-08-01', 1883: '2018-08-01', 1884: '2018-08-01'},\n",
        "    'exp_dt': {667: '2018-07-31', 668: '2018-07-31', 669: '2018-07-31', 670: '2018-07-31', 671: '2018-07-31', 672: '2018-07-31', 673: '2018-07-31',\n",
        "            9: '2018-08-31', 10: '2018-08-31', 17: '2019-02-28', 1881: '9999-12-31', 1882: '9999-12-31', 1883: '9999-12-31', 1884: '9999-12-31'}})\n",
        "demo_audit_cols_ex3 = ['eff_dt', 'exp_dt']\n",
        "\n",
        "print('df')\n",
        "print(demo_df_ex3)\n",
        "print()\n",
        "print('audit_cols')\n",
        "print(demo_audit_cols_ex3)\n",
        "print()\n",
        "print('existing_journal')\n",
        "print(demo_existing_journal_ex3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed5f000b-a024-41bc-aee9-a46e865d4fb4",
      "metadata": {
        "id": "ed5f000b-a024-41bc-aee9-a46e865d4fb4"
      },
      "source": [
        "<!-- Expected demo output text block -->\n",
        "The demo included in the solution cell below should display the following output:\n",
        "```\n",
        "historical_journal WITH NO existing journal\n",
        "Empty DataFrame\n",
        "Columns: [id, paid, service, tier, promo, price, eff_dt, exp_dt]\n",
        "Index: []\n",
        "\n",
        "active_journal WITH NO existing journal\n",
        "Empty DataFrame\n",
        "Columns: [id, paid, service, tier, promo, price, eff_dt, exp_dt]\n",
        "Index: []\n",
        "\n",
        "historical_journal WITH existing journal\n",
        "    id   paid service tier  promo  price      eff_dt      exp_dt\n",
        "667  0   True   video    2  intro  11.99  2018-02-01  2018-07-31\n",
        "668  1   True   audio    1  intro   5.99  2018-02-01  2018-07-31\n",
        "669  2   True   video    2  intro  11.99  2018-02-01  2018-07-31\n",
        "670  2   True   audio    2  intro   9.99  2018-02-01  2018-07-31\n",
        "671  3   True   video    1  intro   8.99  2018-02-01  2018-07-31\n",
        "672  3   True   audio    1  intro   5.99  2018-02-01  2018-07-31\n",
        "673  4   True   audio    1  intro   5.99  2018-02-01  2018-07-31\n",
        "9    3  False   video    1   base  10.99  2018-08-01  2018-08-31\n",
        "10   3  False   audio    1   base   8.99  2018-08-01  2018-08-31\n",
        "17   0   True   video    2   base  15.99  2018-08-01  2019-02-28\n",
        "\n",
        "active_journal WITH existing journal\n",
        "     id  paid service tier promo  price      eff_dt      exp_dt\n",
        "1881  1  True   audio    1  base   8.99  2018-08-01  9999-12-31\n",
        "1882  2  True   video    2  base  15.99  2018-08-01  9999-12-31\n",
        "1883  2  True   audio    2  base  12.99  2018-08-01  9999-12-31\n",
        "1884  4  True   audio    1  base   8.99  2018-08-01  9999-12-31\n",
        "```\n",
        "**Note** - This demo runs your solution two times. The first two DataFrames are the expected result when `exixting_journal` is `None`, and the second two DataFrames are the expected result for the `existing_journal` defined in the cell above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ac3ab59-b66e-43e4-99d1-b24f216a7c46",
      "metadata": {
        "id": "1ac3ab59-b66e-43e4-99d1-b24f216a7c46"
      },
      "outputs": [],
      "source": [
        "outline = '''\n",
        "output:\n",
        "    - (historical_journal, active_journal)\n",
        "        - data_type == tuple of dataframes\n",
        "inputs:\n",
        "    - df\n",
        "        - data_type == DataFrame\n",
        "    - audit_cols\n",
        "        - data_type == list of strings\n",
        "        - will always include eff_dt and exp_dt\n",
        "    - existing_journal\n",
        "        - data_type == df or None\n",
        "        - optional\n",
        "steps:\n",
        "    1. check if existing_journal is None\n",
        "        a. if yes: create empty df where existing_journal cols = df.cols + audit_cols\n",
        "    2. create journals\n",
        "        a. historical_journal = existing_journal where exp_dt != '9999-12-31'\n",
        "        b. active_journal = existing_journal where exp_dt = '9999-12-31'\n",
        "    3. return (historical_journal, active_journal)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b89dd57a-4b8a-4986-877d-bfd9544ca19e",
      "metadata": {
        "id": "b89dd57a-4b8a-4986-877d-bfd9544ca19e",
        "outputId": "574957c4-9634-44d3-f65d-74985f58e84a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "historical_journal WITH NO existing journal\n",
            "Empty DataFrame\n",
            "Columns: [id, paid, service, tier, promo, price, eff_dt, exp_dt]\n",
            "Index: []\n",
            "\n",
            "active_journal WITH NO existing journal\n",
            "Empty DataFrame\n",
            "Columns: [id, paid, service, tier, promo, price, eff_dt, exp_dt]\n",
            "Index: []\n",
            "\n",
            "historical_journal WITH existing journal\n",
            "    id   paid service tier  promo  price      eff_dt      exp_dt\n",
            "667  0   True   video    2  intro  11.99  2018-02-01  2018-07-31\n",
            "668  1   True   audio    1  intro   5.99  2018-02-01  2018-07-31\n",
            "669  2   True   video    2  intro  11.99  2018-02-01  2018-07-31\n",
            "670  2   True   audio    2  intro   9.99  2018-02-01  2018-07-31\n",
            "671  3   True   video    1  intro   8.99  2018-02-01  2018-07-31\n",
            "672  3   True   audio    1  intro   5.99  2018-02-01  2018-07-31\n",
            "673  4   True   audio    1  intro   5.99  2018-02-01  2018-07-31\n",
            "9    3  False   video    1   base  10.99  2018-08-01  2018-08-31\n",
            "10   3  False   audio    1   base   8.99  2018-08-01  2018-08-31\n",
            "17   0   True   video    2   base  15.99  2018-08-01  2019-02-28\n",
            "\n",
            "active_journal WITH existing journal\n",
            "     id  paid service tier promo  price      eff_dt      exp_dt\n",
            "1881  1  True   audio    1  base   8.99  2018-08-01  9999-12-31\n",
            "1882  2  True   video    2  base  15.99  2018-08-01  9999-12-31\n",
            "1883  2  True   audio    2  base  12.99  2018-08-01  9999-12-31\n",
            "1884  4  True   audio    1  base   8.99  2018-08-01  9999-12-31\n"
          ]
        }
      ],
      "source": [
        "### Exercise 3 solution\n",
        "def partition_journal(df, audit_cols, existing_journal=None):\n",
        "    ###\n",
        "    ### YOUR CODE HERE\n",
        "    ###\n",
        "\n",
        "    if existing_journal is None:\n",
        "        df_cols = list(df.columns)\n",
        "        empty_journal_cols = df_cols + audit_cols\n",
        "        existing_journal = pd.DataFrame(columns=empty_journal_cols)\n",
        "\n",
        "    historical_journal = existing_journal[existing_journal['exp_dt'] != '9999-12-31']\n",
        "    active_journal = existing_journal[existing_journal['exp_dt'] == '9999-12-31']\n",
        "\n",
        "    return (historical_journal, active_journal)\n",
        "\n",
        "### demo function call\n",
        "new_hist, new_active = partition_journal(demo_df_ex3, demo_audit_cols_ex3)\n",
        "hist, active = partition_journal(demo_df_ex3, demo_audit_cols_ex3, demo_existing_journal_ex3)\n",
        "print('historical_journal WITH NO existing journal')\n",
        "print(new_hist)\n",
        "print()\n",
        "print('active_journal WITH NO existing journal')\n",
        "print(new_active)\n",
        "print()\n",
        "print('historical_journal WITH existing journal')\n",
        "print(hist)\n",
        "print()\n",
        "print('active_journal WITH existing journal')\n",
        "print(active)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12fb7dc5-0233-4882-86fd-808875e84b76",
      "metadata": {
        "id": "12fb7dc5-0233-4882-86fd-808875e84b76"
      },
      "source": [
        "<!-- Test Cell Boilerplate -->\n",
        "The cell below will test your solution for Exercise 3. The testing variables will be available for debugging under the following names in a dictionary format.\n",
        "- `input_vars` - Input variables for your solution.\n",
        "- `original_input_vars` - Copy of input variables from prior to running your solution. These _should_ be the same as `input_vars` - otherwise the inputs were modified by your solution.\n",
        "- `returned_output_vars` - Outputs returned by your solution.\n",
        "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61c49b7f-4f89-4716-8f8a-f30011011a7f",
      "metadata": {
        "id": "61c49b7f-4f89-4716-8f8a-f30011011a7f",
        "outputId": "0e7ca17f-7754-447f-9b73-017789353a7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This test executed in 5 seconds\n",
            "Passed! Please submit.\n"
          ]
        }
      ],
      "source": [
        "### test_cell_ex3\n",
        "exercise_start = time.time()\n",
        "from tester_fw.testers import Tester\n",
        "\n",
        "conf = {\n",
        "    'case_file':'tc_3',\n",
        "    'func': partition_journal, # replace this with the function defined above\n",
        "    'inputs':{ # input config dict. keys are parameter names\n",
        "        'df':{\n",
        "            'dtype':'pd.DataFrame', # data type of param.\n",
        "            'check_modified':True,\n",
        "        },\n",
        "        'audit_cols':{\n",
        "            'dtype':'list', # data type of param.\n",
        "            'check_modified':True,\n",
        "        },\n",
        "        'existing_journal':{\n",
        "            'dtype':'pd.DataFrame', # data type of param.\n",
        "            'check_modified':True,\n",
        "        }\n",
        "    },\n",
        "    'outputs':{\n",
        "        'historical_journal':{\n",
        "            'index':0,\n",
        "            'dtype':'pd.DataFrame',\n",
        "            'check_dtype': True,\n",
        "            'check_col_dtypes': True, # Ignored if dtype is not df\n",
        "            'check_col_order': False, # Ignored if dtype is not df\n",
        "            'check_row_order': True, # Ignored if dtype is not df\n",
        "            'check_column_type': True, # Ignored if dtype is not df\n",
        "            'float_tolerance': 10 ** (-6)\n",
        "        },\n",
        "        'active_journal':{\n",
        "            'index':1,\n",
        "            'dtype':'pd.DataFrame',\n",
        "            'check_dtype': True,\n",
        "            'check_col_dtypes': True, # Ignored if dtype is not df\n",
        "            'check_col_order': False, # Ignored if dtype is not df\n",
        "            'check_row_order': True, # Ignored if dtype is not df\n",
        "            'check_column_type': True, # Ignored if dtype is not df\n",
        "            'float_tolerance': 10 ** (-6)\n",
        "        }\n",
        "\n",
        "    }\n",
        "}\n",
        "tester = Tester(conf, key=b'6IRWMcPsVIAZqzDJnPgv_MfUZsxqo4Utjm2Favidv-A=', path='resources/')\n",
        "for _ in range(70):\n",
        "    try:\n",
        "        tester.run_test()\n",
        "        (input_vars, original_input_vars, returned_output_vars, true_output_vars) = tester.get_test_vars()\n",
        "    except:\n",
        "        (input_vars, original_input_vars, returned_output_vars, true_output_vars) = tester.get_test_vars()\n",
        "        raise\n",
        "\n",
        "### END HIDDEN TESTS\n",
        "exercise_end = time.time()\n",
        "print(f\"This test executed in {(pd.to_datetime(exercise_end, unit='s') - pd.to_datetime(exercise_start, unit='s')).seconds} seconds\")\n",
        "print('Passed! Please submit.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14a023b8-85fb-4104-b440-831a3d1c2ea0",
      "metadata": {
        "id": "14a023b8-85fb-4104-b440-831a3d1c2ea0"
      },
      "source": [
        "## Exercise 5 - (**3** Points):\n",
        "**Motivation** (don't dwell on this):  \n",
        "Our next task is to identify records which have changed and which records are unchanged. These are a subset of records having keys in both the business data and the active journal. We need to partition both the business data and journal data into two parts based on whether the data has changed.\n",
        "\n",
        "**Requirements**:  \n",
        "Define `compare_changes(compare_new_df, compare_old_df, audit_cols)`.\n",
        "\n",
        "The inputs are as follows:\n",
        "\n",
        "- `compare_new_df` - a DataFrame\n",
        "- `compare_old_df` - another DataFrame with the same columns/shape/indexing as `compare_new_df`\n",
        "- `audit_cols` - a list of column names which should not be used for comparison.\n",
        "\n",
        "You can assume that the rows `compare_new_df` and `compare_old_df` are sorted and indexed such that they can be compared directly.\n",
        "\n",
        "- Identify the columns in `compare_new_df` which are not in `audit_cols`. Let's call this `cols`.\n",
        "- Compare the values in `compare_new_df[cols]` with the values in `compare_old_df[cols]`.\n",
        "- Return these 3 new DataFrames:\n",
        "    - `unchanged` - All of the rows in `compare_new_df`  where **all** values are the same in the comparison.  \n",
        "    - `old_changed` - All of the rows in `compare_old_df` where there are **any** differences in the comparison.\n",
        "    - `new_changed` - All of the rows in `compare_new_df` where there are **any** differences in the comparison.\n",
        "\n",
        "It is possible that `compare_new_df` and `compare_old_df` are **both** empty DataFrames. If this is the case all 3 returned DataFrames would also be empty.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb1c051d-2c42-495f-8ae7-ffaf40935806",
      "metadata": {
        "id": "cb1c051d-2c42-495f-8ae7-ffaf40935806",
        "outputId": "c7e61188-9b4f-4e8e-cbe1-41e4ad7e88a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "compare_new_df\n",
            "  some_column   key_column audit_column_1 audit_column_2\n",
            "0   new_val_0    changed_0           None           None\n",
            "1   new_val_1    changed_1           None           None\n",
            "2  same_val_0  unchanged_0           None           None\n",
            "3  same_val_1  unchanged_1           None           None\n",
            "4   new_val_2    changed_2           None           None\n",
            "5  same_val_2  unchanged_2           None           None\n",
            "\n",
            "compare_old_df\n",
            "  some_column   key_column audit_column_1 audit_column_2\n",
            "0   old_val_0    changed_0            foo            bar\n",
            "1   old_val_1    changed_1            foo            bar\n",
            "2  same_val_0  unchanged_0            foo            bar\n",
            "3  same_val_1  unchanged_1            foo            bar\n",
            "4   old_val_2    changed_2            foo            bar\n",
            "5  same_val_2  unchanged_2            foo            bar\n",
            "\n",
            "audit_cols\n",
            "['audit_column_1', 'audit_column_2']\n"
          ]
        }
      ],
      "source": [
        "### Define demo inputs\n",
        "\n",
        "demo_compare_new_df_ex5 = pd.DataFrame([\n",
        "    {'some_column': 'new_val_0', 'key_column': 'changed_0', 'audit_column_1': None, 'audit_column_2': None},\n",
        "    {'some_column': 'new_val_1', 'key_column': 'changed_1', 'audit_column_1': None, 'audit_column_2': None},\n",
        "    {'some_column': 'same_val_0', 'key_column': 'unchanged_0', 'audit_column_1': None, 'audit_column_2': None},\n",
        "    {'some_column': 'same_val_1', 'key_column': 'unchanged_1', 'audit_column_1': None, 'audit_column_2': None},\n",
        "    {'some_column': 'new_val_2', 'key_column': 'changed_2', 'audit_column_1': None, 'audit_column_2': None},\n",
        "    {'some_column': 'same_val_2', 'key_column': 'unchanged_2', 'audit_column_1': None, 'audit_column_2': None}\n",
        "])\n",
        "\n",
        "demo_compare_old_df_ex5 = pd.DataFrame([\n",
        "    {'some_column': 'old_val_0', 'key_column': 'changed_0', 'audit_column_1': 'foo', 'audit_column_2': 'bar'},\n",
        "    {'some_column': 'old_val_1', 'key_column': 'changed_1', 'audit_column_1': 'foo', 'audit_column_2': 'bar'},\n",
        "    {'some_column': 'same_val_0', 'key_column': 'unchanged_0', 'audit_column_1': 'foo', 'audit_column_2': 'bar'},\n",
        "    {'some_column': 'same_val_1', 'key_column': 'unchanged_1', 'audit_column_1': 'foo', 'audit_column_2': 'bar'},\n",
        "    {'some_column': 'old_val_2', 'key_column': 'changed_2', 'audit_column_1': 'foo', 'audit_column_2': 'bar'},\n",
        "    {'some_column': 'same_val_2', 'key_column': 'unchanged_2', 'audit_column_1': 'foo', 'audit_column_2': 'bar'}\n",
        "])\n",
        "\n",
        "demo_audit_cols_ex5 = ['audit_column_1', 'audit_column_2']\n",
        "\n",
        "print('compare_new_df')\n",
        "print(demo_compare_new_df_ex5)\n",
        "print()\n",
        "print('compare_old_df')\n",
        "print(demo_compare_old_df_ex5)\n",
        "print()\n",
        "print('audit_cols')\n",
        "print(demo_audit_cols_ex5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f904128b-3d48-4743-b993-ec0cfaddbd73",
      "metadata": {
        "id": "f904128b-3d48-4743-b993-ec0cfaddbd73"
      },
      "outputs": [],
      "source": [
        "outline = '''\n",
        "output:\n",
        "    - (unchanged, old_changed, new_changed)\n",
        "        - data_type == DataFrames\n",
        "inputs:\n",
        "    - compare_new_df\n",
        "        - data_type == DataFrame\n",
        "    - compare_old_df\n",
        "        - data_type == DataFrame\n",
        "    - audit_cols\n",
        "        - data_type == list\n",
        "steps:\n",
        "    1. check to see if input dfs are empty\n",
        "        a. if empty, return 3 empty DataFrames with the right columns names\n",
        "        b. do this by copying compare_new_df 3 times\n",
        "    2. find cols that are not in audit_cols\n",
        "        a. get list of columns from compare_new_cols\n",
        "        b. if col in compare_new_cols not in audit_cols:\n",
        "            - add to new list, cols\n",
        "    3. create boolean mask - True when there is any difference between the two dfs[cols]\n",
        "    4. use different to partition the dfs\n",
        "    5. return (unchanged, old_changed, new_changed)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b8f5eb5-acca-4b8d-b501-daced35436ea",
      "metadata": {
        "id": "1b8f5eb5-acca-4b8d-b501-daced35436ea",
        "outputId": "f72a9f63-8f41-4539-ab17-3bb58d7b09b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "unchanged\n",
            "  some_column   key_column audit_column_1 audit_column_2\n",
            "2  same_val_0  unchanged_0           None           None\n",
            "3  same_val_1  unchanged_1           None           None\n",
            "5  same_val_2  unchanged_2           None           None\n",
            "\n",
            "old_changed\n",
            "  some_column key_column audit_column_1 audit_column_2\n",
            "0   old_val_0  changed_0            foo            bar\n",
            "1   old_val_1  changed_1            foo            bar\n",
            "4   old_val_2  changed_2            foo            bar\n",
            "\n",
            "new_changed\n",
            "  some_column key_column audit_column_1 audit_column_2\n",
            "0   new_val_0  changed_0           None           None\n",
            "1   new_val_1  changed_1           None           None\n",
            "4   new_val_2  changed_2           None           None\n"
          ]
        }
      ],
      "source": [
        "### Exercise 5 solution\n",
        "def compare_changes(compare_new_df, compare_old_df, audit_cols):\n",
        "    ###\n",
        "    ### YOUR CODE HEREa\n",
        "    ###\n",
        "\n",
        "    # check for empty inputs\n",
        "    if compare_new_df.shape[0] == compare_old_df.shape[0] == 0:\n",
        "        return (compare_new_df.copy(), compare_new_df.copy(), compare_new_df.copy())\n",
        "\n",
        "    # find cols that are not in audit_cols\n",
        "    compare_new_cols = list(compare_new_df.columns)\n",
        "    cols = []\n",
        "    for i in compare_new_cols:\n",
        "        if i not in audit_cols:\n",
        "            cols.append(i)\n",
        "\n",
        "    # create boolean mask - True when there is any difference between the two frames, ignoring audit_cols\n",
        "    different = (compare_new_df[cols] != compare_old_df[cols]).any(axis=1)\n",
        "\n",
        "    # use different to partition the dfs\n",
        "    unchanged = compare_new_df.loc[~different, :]\n",
        "    old_changed = compare_old_df.loc[different, :]\n",
        "    new_changed = compare_new_df.loc[different, :]\n",
        "\n",
        "    # return\n",
        "    return (unchanged, old_changed, new_changed)\n",
        "\n",
        "# Run demo of function\n",
        "(demo_unchanged_ex5,\n",
        "demo_old_changed_ex5,\n",
        "demo_new_changed_ex5) = compare_changes(demo_compare_new_df_ex5, demo_compare_old_df_ex5, demo_audit_cols_ex5)\n",
        "print('unchanged')\n",
        "print(demo_unchanged_ex5)\n",
        "print()\n",
        "print('old_changed')\n",
        "print(demo_old_changed_ex5)\n",
        "print()\n",
        "print('new_changed')\n",
        "print(demo_new_changed_ex5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90f1527a-d5b2-4d48-827c-eca7d7f17dbf",
      "metadata": {
        "id": "90f1527a-d5b2-4d48-827c-eca7d7f17dbf",
        "outputId": "d0bb4226-e61d-440a-94de-198a9c0e5632"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This test executed in 5 seconds\n",
            "Passed! Please submit.\n"
          ]
        }
      ],
      "source": [
        "### test_cell_ex5\n",
        "exercise_start = time.time()\n",
        "from tester_fw.testers import Tester\n",
        "\n",
        "conf = {\n",
        "    'case_file':'tc_5',\n",
        "    'func': compare_changes, # replace this with the function defined above\n",
        "    'inputs':{ # input config dict. keys are parameter names\n",
        "        'compare_new_df':{\n",
        "            'dtype':'pd.DataFrame', # data type of param.\n",
        "            'check_modified':True,\n",
        "        },\n",
        "        'compare_old_df':{\n",
        "            'dtype':'pd.DataFrame', # data type of param.\n",
        "            'check_modified':True,\n",
        "        },\n",
        "        'audit_cols':{\n",
        "            'dtype':'list', # data type of param.\n",
        "            'check_modified':True,\n",
        "        }\n",
        "    },\n",
        "    'outputs':{\n",
        "        'unchanged':{\n",
        "            'index':0,\n",
        "            'dtype':'pd.DataFrame',\n",
        "            'check_dtype': True,\n",
        "            'check_col_dtypes': False, # Ignored if dtype is not df\n",
        "            'check_col_order': False, # Ignored if dtype is not df\n",
        "            'check_row_order': False, # Ignored if dtype is not df\n",
        "            'check_column_type': True, # Ignored if dtype is not df\n",
        "            'float_tolerance': 10 ** (-6)\n",
        "        },\n",
        "        'old_changed':{\n",
        "            'index':1,\n",
        "            'dtype':'pd.DataFrame',\n",
        "            'check_dtype': True,\n",
        "            'check_col_dtypes': False, # Ignored if dtype is not df\n",
        "            'check_col_order': False, # Ignored if dtype is not df\n",
        "            'check_row_order': False, # Ignored if dtype is not df\n",
        "            'check_column_type': True, # Ignored if dtype is not df\n",
        "            'float_tolerance': 10 ** (-6)\n",
        "        },\n",
        "        'new_changed':{\n",
        "            'index':2,\n",
        "            'dtype':'pd.DataFrame',\n",
        "            'check_dtype': True,\n",
        "            'check_col_dtypes': False, # Ignored if dtype is not df\n",
        "            'check_col_order': False, # Ignored if dtype is not df\n",
        "            'check_row_order': False, # Ignored if dtype is not df\n",
        "            'check_column_type': True, # Ignored if dtype is not df\n",
        "            'float_tolerance': 10 ** (-6)\n",
        "        }\n",
        "    }\n",
        "}\n",
        "tester = Tester(conf, key=b'6IRWMcPsVIAZqzDJnPgv_MfUZsxqo4Utjm2Favidv-A=', path='resources/')\n",
        "for _ in range(70):\n",
        "    try:\n",
        "        tester.run_test()\n",
        "        (input_vars, original_input_vars, returned_output_vars, true_output_vars) = tester.get_test_vars()\n",
        "    except:\n",
        "        (input_vars, original_input_vars, returned_output_vars, true_output_vars) = tester.get_test_vars()\n",
        "        raise\n",
        "\n",
        "exercise_end = time.time()\n",
        "print(f\"This test executed in {(pd.to_datetime(exercise_end, unit='s') - pd.to_datetime(exercise_start, unit='s')).seconds} seconds\")\n",
        "print('Passed! Please submit.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59cef587-5fff-47e4-8d2f-24a6b23990bc",
      "metadata": {
        "id": "59cef587-5fff-47e4-8d2f-24a6b23990bc"
      },
      "source": [
        "**Fin!** Youve reached the end of this part. Dont forget to restart and run all cells again to make sure its all working when run in sequence; and make sure your work passes the submission process. Good luck!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}